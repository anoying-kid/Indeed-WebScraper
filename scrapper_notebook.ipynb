{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24986788",
   "metadata": {},
   "source": [
    "# This is a program for scrapping the jobs from the site indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf060b7",
   "metadata": {},
   "source": [
    "## Library used\n",
    "    * bs4 or Beautifulsoup popular for scraping\n",
    "        * lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python.\n",
    "    * asyncio for asynchronous programming\n",
    "    * aiohttp library used for getting aynchronous request.\n",
    "    * tqdm is used for the getting bars in iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801f74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582737d3",
   "metadata": {},
   "source": [
    "## IndeedScraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e4784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndeedScraper():\n",
    "    \n",
    "    # For storing the request\n",
    "    li= []\n",
    "    \n",
    "    # For storing the data\n",
    "    di= {}\n",
    "    \n",
    "    # job variable is used for naming dictionary( di ) key\n",
    "    job=1\n",
    "    \n",
    "    # Constructor getting the required data\n",
    "    def __init__(self, what, where, page_till):\n",
    "        self.what= what\n",
    "        self.where= where\n",
    "        self.page_till= page_till\n",
    "    \n",
    "    \n",
    "    # This will request the website for getting the page content\n",
    "    def get_task(self, s):\n",
    "        task= []\n",
    "        for page in range(self.page_till):\n",
    "            page= page*10\n",
    "            \n",
    "            # Page url\n",
    "            url= 'https://in.indeed.com/jobs'\n",
    "            \n",
    "            # Url parameters\n",
    "            query= {\n",
    "                'q': self.what,\n",
    "                'l': self.where,\n",
    "                'start': page\n",
    "            }\n",
    "            \n",
    "            # HTTP headers are widely used during web scraping because they allow access to otherwise blocked information.\n",
    "            headers= {\"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.1 Safari/605.1.15\"}\n",
    "            \n",
    "            # Appending all the request in task list\n",
    "            task.append(s.get(url, params=query, headers=headers, ssl=False))\n",
    "        return task\n",
    "\n",
    "    \n",
    "    # req() function will ascynchronously request the data from website\n",
    "    async def req(self):\n",
    "        \n",
    "        # Creating the session\n",
    "        async with aiohttp.ClientSession() as s:\n",
    "            tasks= self.get_task(s)\n",
    "            responses= await asyncio.gather(*tasks)\n",
    "            \n",
    "            # tqdm() function is used for bars\n",
    "            for response in tqdm(responses, desc=\"Extracting Data!\"):\n",
    "                \n",
    "                # appending() all the request in li list\n",
    "                self.li.append(await response.text())\n",
    "                \n",
    "    # Try and except is used since some of the detail like companyName, companyReq, dateposted aren't there so\n",
    "    ## instead of getting the error we print jobs without them\n",
    "    \n",
    "    # title() function will scrap the title of the jobs\n",
    "    async def title(self, text, page, job):\n",
    "        soup= BeautifulSoup(text, 'lxml')\n",
    "        Title= soup.find_all('td', {\"class\": \"resultContent\"})\n",
    "        for title in Title:\n",
    "            self.di[f'job{page}{job}']={}\n",
    "            ti = title.find_next('span').text\n",
    "            if ti=='new':\n",
    "                self.di[f'job{page}{job}']['title']=title.find_all('span')[1].text\n",
    "            else:\n",
    "                self.di[f'job{page}{job}']['title']=ti\n",
    "            job+=1\n",
    "    \n",
    "    # companyName() function will scrap the company name of the jobs\n",
    "    async def companyName(self, text, page, job):\n",
    "        soup=BeautifulSoup(text, 'lxml')\n",
    "        Title= soup.find_all('div', {'class': 'heading6 company_location tapItem-gutter companyInfo'})\n",
    "        for title in Title:\n",
    "            try:\n",
    "                self.di[f'job{page}{job}']['companyName']= title.find('span',{'class':'companyName'}).text\n",
    "                self.di[f'job{page}{job}']['companyLocation']= title.find('div',{'class':'companyLocation'}).text\n",
    "                job+=1\n",
    "            except KeyError:\n",
    "                pass\n",
    "    # companyReq() function will scrap the company requirements of the jobs\n",
    "    async def companyReq(self, text, page, job):\n",
    "        soup=BeautifulSoup(text, 'lxml')\n",
    "        Title= soup.find_all('div', attrs={'class': 'job-snippet'})\n",
    "        for title in Title:\n",
    "            try:\n",
    "                self.di[f'job{page}{job}']['companyReq']= f\"1) {title.ul.li.text}.\\n2) {title.ul.li.find_next('li').text}\"\n",
    "                job+=1\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            \n",
    "    # datePosted() function will scrap the date when posted\n",
    "    async def datePosted(self, text, page, job):\n",
    "        soup=BeautifulSoup(text, 'lxml')\n",
    "        Title= soup.find_all('span', attrs={'class': 'date'})\n",
    "        for title in Title:\n",
    "            try:\n",
    "                self.di[f'job{page}{job}']['datePosted']= title.text\n",
    "                job+=1\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "    # getting all the stuff in one place\n",
    "    async def main(self):\n",
    "        page=1\n",
    "        \n",
    "        # This is done to follow DRY- Don't Repeat Yourself, we have same lines again and again so getattr is used.\n",
    "        function= [\"title\", \"companyName\", \"companyReq\", \"datePosted\"]\n",
    "\n",
    "        tasks=[]\n",
    "        \n",
    "        for fun in tqdm(function, desc=\"Clearing Data...\"):\n",
    "            for text in self.li:\n",
    "                \n",
    "                # getattr() method returns the value of the named attribute of an object\n",
    "                tasks.append(asyncio.create_task(getattr(self, fun)(text, page, self.job) ))\n",
    "                page+=1\n",
    "            await asyncio.gather(*tasks)\n",
    "            page=1\n",
    "\n",
    "        ## We can print this terminal also\n",
    "        # for i in self.di:\n",
    "        #     print('\\n')\n",
    "        #     for j in self.di[i]:\n",
    "        #         print(f'{j} : {self.di[i][j]}')\n",
    "        #     print('-----------------------\\n')\n",
    "        \n",
    "        # also saving the data in txt file\n",
    "        with open('data.txt', 'w') as w:\n",
    "            ls = []\n",
    "            for i in self.di:\n",
    "                ls.append('\\n')\n",
    "                for j in self.di[i]:\n",
    "                    ls.append(f\"{j.capitalize()} : {self.di[i][j].capitalize()}\\n\")\n",
    "                ls.append('-----------------------\\n')\n",
    "            w.write(''.join(ls))\n",
    "\n",
    "        print(f\"Number of jobs available {len(self.di)}\")\n",
    "\n",
    "## Uncomment these lines if running in ide not using jupyter\n",
    "#     def run(self):\n",
    "#         asyncio.run(self.req())\n",
    "#         asyncio.run(self.main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e981f8f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your profession, skill: python\n",
      "Where you want your job: delhi\n",
      "Number of page to extract: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Data!: 100%|███████████████████████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "Clearing Data...: 100%|███████████████████████████| 4/4 [00:00<00:00, 32.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of jobs available 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skill= input(\"Your profession, skill: \")\n",
    "place= input(\"Where you want your job: \")\n",
    "page_till=int(input(\"Number of page to extract: \"))\n",
    "indeed = IndeedScraper(skill, place, page_till)\n",
    "await indeed.req()\n",
    "await indeed.main()\n",
    "\n",
    "## Uncomment these lines if running in ide not using jupyter\n",
    "# indeed.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe702a",
   "metadata": {},
   "source": [
    "## creating our own filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08023109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs won't be shown with your skill you won't know\n",
    "def unknown_skill(di, word):\n",
    "    di_unknown = {}\n",
    "    for i,j in di.items():\n",
    "        if word not in j['companyReq'] and word not in j['title']:\n",
    "            di_unknown[i] = j\n",
    "    return di_unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8359a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs will be shown with skill you know\n",
    "def known_skill(di, word):\n",
    "    di_known = {}\n",
    "    for i,j in di.items():\n",
    "        if word in j['companyReq'] or word in j['title']:\n",
    "            di_known[i] = j\n",
    "    return di_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48682cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = known_skill(indeed.di, 'AWS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "824b898b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:Cybersecurity System Engineer (APAC)\n",
      "\n",
      "companyName:D3 Security\n",
      "\n",
      "companyLocation:Remote in New Delhi, Delhi\n",
      "\n",
      "companyReq:1) The primary responsibilities will be to assist Clients in planning, design & implementation of their Cyber Security Incident Response system..\n",
      "2) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology.\n",
      "\n",
      "datePosted:EmployerActive 2 days ago\n",
      "\n",
      "\n",
      "\n",
      "title:Software Developer, Trilogy (Remote) - $100,000/year USD\n",
      "\n",
      "companyName:Crossover\n",
      "\n",
      "companyLocation:Remote in New Delhi, Delhi\n",
      "\n",
      "companyReq:1) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology..\n",
      "2) Ability to work 100% remotely.\n",
      "\n",
      "datePosted:Posted4 days ago\n",
      "\n",
      "\n",
      "\n",
      "title:Senior Software Developer, Trilogy (Remote) - $100,000/year...\n",
      "\n",
      "companyName:Crossover\n",
      "\n",
      "companyLocation:Remote in New Delhi, Delhi\n",
      "\n",
      "companyReq:1) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology..\n",
      "2) Ability to work 100% remotely.\n",
      "\n",
      "datePosted:Posted4 days ago\n",
      "\n",
      "\n",
      "\n",
      "title:Software Engineer, Trilogy (Remote) - $100,000/year USD\n",
      "\n",
      "companyName:Crossover\n",
      "\n",
      "companyLocation:Remote in New Delhi, Delhi\n",
      "\n",
      "companyReq:1) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology..\n",
      "2) Ability to work 100% remotely.\n",
      "\n",
      "datePosted:Posted4 days ago\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing required data\n",
    "for i in data.values():\n",
    "    for j,k in i.items():\n",
    "        print(f'{j}:{k}\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59be917d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job11': {'title': 'Cybersecurity System Engineer (APAC)',\n",
       "  'companyName': 'D3 Security',\n",
       "  'companyLocation': 'Remote in New Delhi, Delhi',\n",
       "  'companyReq': '1) The primary responsibilities will be to assist Clients in planning, design & implementation of their Cyber Security Incident Response system..\\n2) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology.',\n",
       "  'datePosted': 'EmployerActive 2 days ago'},\n",
       " 'job12': {'title': 'Software Developer, Trilogy (Remote) - $100,000/year USD',\n",
       "  'companyName': 'Crossover',\n",
       "  'companyLocation': 'Remote in New Delhi, Delhi',\n",
       "  'companyReq': '1) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology..\\n2) Ability to work 100% remotely.',\n",
       "  'datePosted': 'Posted4 days ago'},\n",
       " 'job13': {'title': 'Hiring Freshers @Fortune500 Company -Gurgaon',\n",
       "  'companyName': 'Net Connect',\n",
       "  'companyLocation': 'Delhi, Delhi',\n",
       "  'companyReq': '1) Good understanding of C++,Java, Python, Data Structure, Javascript, Angular, React..\\n2) Training on Any Technology (Java, C++, Python, Data Science, Full Stack).',\n",
       "  'datePosted': 'Posted4 days ago'},\n",
       " 'job14': {'title': 'Hiring fresher for Python Development',\n",
       "  'companyName': 'Wildnet Technologies',\n",
       "  'companyLocation': 'Remote in Delhi, Delhi',\n",
       "  'companyReq': '1) Developing back-end components to improve responsiveness and overall performance..\\n2) Integrating user-facing elements into applications.',\n",
       "  'datePosted': 'EmployerActive 2 days ago'},\n",
       " 'job15': {'title': 'Senior Software Developer, Trilogy (Remote) - $100,000/year...',\n",
       "  'companyName': 'Crossover',\n",
       "  'companyLocation': 'Remote in New Delhi, Delhi',\n",
       "  'companyReq': '1) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology..\\n2) Ability to work 100% remotely.',\n",
       "  'datePosted': 'Posted4 days ago'},\n",
       " 'job16': {'title': 'Software Engineer, Trilogy (Remote) - $100,000/year USD',\n",
       "  'companyName': 'Crossover',\n",
       "  'companyLocation': 'Remote in New Delhi, Delhi',\n",
       "  'companyReq': '1) We aim to build carefully-architected planet-scale products with very few(<5k) lines of code, directly on top of AWS technology..\\n2) Ability to work 100% remotely.',\n",
       "  'datePosted': 'Posted4 days ago'},\n",
       " 'job17': {'title': 'Python Developer (junior)',\n",
       "  'companyName': 'FENG GROUP',\n",
       "  'companyLocation': 'New Delhi, Delhi',\n",
       "  'companyReq': '1) Our client is looking for a Junior Python Developer to assist senior resources and write Python code which is efficient and modular server-side code..\\n2) View all FENG GROUP jobs - New Delhi jobs',\n",
       "  'datePosted': 'Posted2 days ago'},\n",
       " 'job18': {'title': 'Python Developer',\n",
       "  'companyName': 'Pinnacle Digital Analytics (P) Ltd',\n",
       "  'companyLocation': 'Delhi, Delhi',\n",
       "  'companyReq': '1) Experience : 3 to 5 Years..\\n2) Working Env : Linux / Unix / Centos.',\n",
       "  'datePosted': 'Posted3 days ago'},\n",
       " 'job19': {'title': 'JavaScript/Python Developer Fresher',\n",
       "  'companyName': 'Summence Technologies',\n",
       "  'companyLocation': 'Dwarka, Delhi, Delhi',\n",
       "  'companyReq': '1) Candidate should have basic knowledge of SQL..\\n2) Candidate should have knowledge of Python, JavaScript, CSS3, HTML5.',\n",
       "  'datePosted': 'EmployerActive 12 days ago'},\n",
       " 'job110': {'title': 'Python Trainer & Data Science Trainer',\n",
       "  'companyName': 'Jeetech Academy',\n",
       "  'companyLocation': 'Delhi, Delhi',\n",
       "  'companyReq': '1) We are looking for experienced faculty Should have exposure to Python, data analytics, data science, Machine learning, Deep Learning, SQL, Tableau, logistic….\\n2) View all Jeetech Academy jobs - Delhi jobs',\n",
       "  'datePosted': 'Posted3 days ago'},\n",
       " 'job111': {'title': 'Python Developer',\n",
       "  'companyName': 'Hestabit Technologies Private Limited',\n",
       "  'companyLocation': 'Delhi, Delhi',\n",
       "  'companyReq': '1) Good knowledge of Python development..\\n2) Technically Sound and Expert in Django and Flask.',\n",
       "  'datePosted': 'EmployerActive 2 days ago'},\n",
       " 'job112': {'title': 'Programming Faculty for C ,C++ | Java , Python',\n",
       "  'companyName': 'Brahmanand institute of Information technology',\n",
       "  'companyLocation': 'New Delhi, Delhi',\n",
       "  'companyReq': '1) Candidate must have MCA OR other diploma related programming..\\n2) Candidate should 2-3 years of experience of teaching programming C& C++ | jAVA | PYTHON.',\n",
       "  'datePosted': 'Posted2 days ago'},\n",
       " 'job113': {'title': 'Machine Learning Engineer',\n",
       "  'companyName': 'Intellekt AI LLP',\n",
       "  'companyLocation': 'Remote in Delhi, Delhi',\n",
       "  'companyReq': '1) Full time permanent remote position..\\n2) Working on machine and deep learning projects.',\n",
       "  'datePosted': 'Posted3 days ago'},\n",
       " 'job114': {'title': 'Coding Teacher',\n",
       "  'companyName': 'Super AI Polaris',\n",
       "  'companyLocation': 'Remote in New Delhi, Delhi',\n",
       "  'companyReq': '1) In our ed-tech vertical we are looking for part time coding teachers..\\n2) Conducting demos for new partners and students.',\n",
       "  'datePosted': 'EmployerActive 3 days ago'},\n",
       " 'job115': {'title': 'Backend Developer',\n",
       "  'companyName': 'Roadcast Tech. Solutions Pvt. Ltd.',\n",
       "  'companyLocation': 'New Delhi, Delhi',\n",
       "  'companyReq': '1) Who has desire to work in a start-up environment, Fun@work culture, and able to self-manage..\\n2) Be proficient in using version control and continuous integration,…',\n",
       "  'datePosted': 'EmployerActive 2 days ago'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed.di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e7f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b15157bbf0b90143226f00c63263d7c548c78ff8835dfd1777e63cce64135c20"
  },
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
